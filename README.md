# AI Dataset Cleaner



This project implements an **AI‑assisted dataset cleaning tool**.  It takes
messy JSON or CSV files and automatically detects schema mismatches,
missing values, duplicates, type inconsistencies and category
inconsistencies.  It then applies sensible fixes to produce a clean,
analysis‑ready dataset.  The tool also generates a summary report of
the issues found and the actions taken to fix them.

The goal is to reduce the time analysts spend wrangling data and to
provide consistent, reproducible cleaning results.  It draws on
established data quality best practices: checking expected columns,
standardising data types, handling missing values, removing duplicates,
standardising categories and formats, and documenting every change.

## Features

### Schema mismatch detection

Datasets often arrive with columns missing or renamed.  This tool
accepts an optional **schema definition** (a JSON file listing
expected column names and data types) and compares it against the
 columns in the dataset.  Missing columns and unexpected columns are
 reported.  When a column is missing the tool adds it with a default
 value (e.g. `NaN`).  For unexpected columns the cleaner attempts to
 **rename** them to the expected name if the normalised forms match
 (ignoring case and punctuation) or if a simple synonym mapping
 applies – for example `Years` is mapped to `age` and `Pay` is mapped to
 `salary`.  Otherwise unexpected columns are retained by default or
 dropped when strict mode is enabled.  Checking columns against a schema
 is essential because missing or unexpected columns
break downstream pipelines【234369309510859†L78-L93】.  Pandera’s
documentation notes that when schema columns are required,
missing columns raise an error【631586573974689†L470-L480】, and
strict schemas can drop unexpected columns【631586573974689†L610-L646】; this
tool follows those principles.

### Data type conversion

Real‑world data often stores numeric values as strings or dates as
objects.  The cleaner inspects each column’s inferred type and
attempts to coerce it to the expected type: numeric columns are
converted with `pd.to_numeric` and date columns with
`pd.to_datetime`, following the recommended practice of casting
types to their correct form【234369309510859†L102-L117】.  After
conversion, it verifies types with `df.dtypes` and logs any
columns that could not be converted.  Standardising types reduces
schema mismatches and makes subsequent analysis more reliable.

### Missing value handling

Missing data is one of the most common issues in messy datasets.  The
tool first identifies missing values using `isnull().sum()`【534126459358432†L829-L836】.
It then fills missing numeric values with the column **median** and fills
categorical values with the mode.  Using the median is more robust to
outliers than the mean.  This aligns with common data
 cleaning strategies: filling missing values using `fillna()` with a
 specific value or mean/median/mode imputation【534126459358432†L837-L843】.
 Best practices emphasise examining the
 pattern of missing values, documenting the chosen strategy, and
 validating results【534126459358432†L846-L854】; the report generated by
the cleaner records all missing value treatments.

### Duplicate and inconsistency detection

Duplicate rows can distort statistics and model training.  The
cleaner uses `duplicated()` to identify duplicates and
`drop_duplicates()` to remove them【534126459358432†L905-L917】.  It reports
the number of duplicates removed and allows the user to specify
columns that determine uniqueness in future versions.  Some duplicates may represent
legitimate data points【534126459358432†L919-L924】, so duplicate removal can be
disabled via the ``--no-drop-duplicates`` flag if desired.

Inconsistent categories (e.g. `'Male'`, `'male'`, `'M'`) and
non‑standardised text (extra whitespace or inconsistent case) are
common problems.  The tool normalises text data by trimming
whitespace, converting to lowercase, and mapping synonyms to a
canonical value.  Examples from the data cleaning guide show how
case sensitivity and whitespace can be resolved with `.str.strip()`
and `.str.title()`【234369309510859†L170-L177】, and how categorical
inconsistencies can be mapped to standard labels【234369309510859†L190-L199】.
Date formats are standardised using `pd.to_datetime()`【234369309510859†L181-L186】.

### Automatic fixing and summary report

After detecting issues, the cleaner applies fixes:

* **Column addition/renaming/dropping** based on the schema.
* **Type coercion** using pandas’ conversion functions.
* **Missing value imputation** using mean for numeric data and mode
  for categorical data.
* **Duplicate removal** when requested, using `drop_duplicates()`【534126459358432†L905-L917】.
* **Category standardisation** to unify inconsistent values【234369309510859†L190-L199】.

Every fix is logged.  The summary report lists initial issues,
actions taken, and the final schema.  For large files, the tool
processes data in chunks to reduce memory usage, following advice to
process large datasets in chunks using `pd.read_csv(..., chunksize=...)` and
apply vectorised operations for efficiency【534126459358432†L1016-L1043】.

## Installation

1. Clone this repository or download the source.
2. Create a virtual environment (recommended):

   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

This project depends on `pandas` and `numpy` for data manipulation and
uses only Python’s standard library for fuzzy matching and CLI parsing.

## Usage

Run the cleaner on a CSV file:

```bash
python -m ai_dataset_cleaner.cli --input data.csv --schema schema.json
```

Run on a JSON file and **retain** duplicates:

```bash
python -m ai_dataset_cleaner.cli --input data.json --no-drop-duplicates
```

Key options:

* `--input`: Path to the CSV or JSON file to clean (required).
* `--schema`: Optional path to a JSON schema file specifying expected
  columns and data types.  If omitted, the cleaner infers the
  schema from the data.
* `--output`: Path to save the cleaned file (default is derived from the input path).
* `--no-drop-duplicates`: By default the cleaner removes duplicate rows using `drop_duplicates()`.  Passing
  this flag disables duplicate removal.
* `--strict`: Drop columns not present in the schema.  Without this
  flag, unexpected columns are retained.
* `--preview`: Print a preview of the cleaned data (first five rows) in addition to the JSON summary report.

The schema file should be a JSON object with two keys:

```json
{
  "columns": ["id", "name", "age", "salary"],
  "dtypes": {"id": "int", "age": "int", "salary": "float", "name": "str"}
}
```

Columns listed under `columns` are considered required.  The
`dtypes` mapping is optional; if provided, the cleaner attempts to
coerce columns to these types.

## Example

Suppose `employees.csv` contains the following messy data:

```csv
ID,Name,Years,Pay
101, Alice , 25,"$50,000"
102,bob,,50000
103,Charlie,30,50000
103,Charlie,30,50000

```

And the expected schema (`schema.json`) is:

```json
{
  "columns": ["id", "name", "age", "salary"],
  "dtypes": {"id": "int", "age": "int", "salary": "float"}
}
```

Running the cleaner:

```bash
python -m ai_dataset_cleaner.cli --input employees.csv --schema schema.json
```

Produces `employees_cleaned.csv` with columns in the expected order and
types:

```csv
id,name,age,salary
101,Alice,25,50000.0
102,Bob,27,50000.0
103,Charlie,30,50000.0
```

The report will mention:

* Renamed columns (`ID` → `id`, `Name` → `name`, `Years` → `age`, `Pay` → `salary`).
* Added missing column `age` with default values.
* Converted `salary` to numeric using `pd.to_numeric`【234369309510859†L102-L117】.
* Filled missing age with the column mean.
* Removed duplicate row using `drop_duplicates()`【534126459358432†L905-L917】.
* Trimmed whitespace and standardised names【234369309510859†L170-L177】.

## Limitations and future work

Although the cleaner automates many common data cleaning tasks, it
cannot infer business rules or domain‑specific transformations.  It
operates on generic heuristics and may require manual intervention for
complex cases.  Future extensions could integrate schema validation
libraries such as Pandera, support AI‑based imputation, and provide
interactive suggestions for ambiguous fixes.

## References

* **Column and schema checks** – The “No‑Nonsense Guide to Cleaning
  Dirty Data” explains that missing columns and unexpected columns can
  break your data pipeline and demonstrates how to detect them by
  comparing the dataframe’s columns to an expected list【234369309510859†L78-L93】.
* **Schema enforcement** – Pandera’s documentation shows that
  required schema columns raise an error when missing【631586573974689†L470-L480】
  and that strict schemas drop unexpected columns【631586573974689†L610-L646】.
* **Data type casting** – Converting object columns to numeric or
  datetime types using `pd.to_numeric` and `pd.to_datetime` is
  recommended to ensure correct data types【234369309510859†L102-L117】.
* **Missing value strategies** – Data cleaning guides suggest using
  `isnull().sum()` to identify missing values and filling them
  using `fillna()` with specific values or mean/median/mode
  imputation【534126459358432†L829-L857】.
* **Duplicate removal** – Duplicates can be identified with
  `duplicated()` and removed with `drop_duplicates()`【534126459358432†L905-L917】.
* **Text and category cleaning** – Normalising case, stripping
  whitespace【234369309510859†L170-L177】, standardising dates【234369309510859†L181-L186】, and
  mapping categorical inconsistencies to canonical values【234369309510859†L190-L199】 are
  essential steps in data cleaning.
